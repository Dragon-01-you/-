import os
import time
import logging
import sys
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import CharacterTextSplitter
from langchain_core.documents import Document

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')
logger = logging.getLogger(__name__)

# ========== å…³é”®é…ç½® ==========
VECTOR_DB_PATH = "./vector_db"  # å‘é‡åº“å­˜å‚¨è·¯å¾„
DATA_FOLDER = "æ±Ÿè¥¿å·¥ä¸šå·¥ç¨‹èŒä¸šæŠ€æœ¯å­¦é™¢_æ•°æ®ä»“åº“"  # æ•°æ®ä»“åº“æ ¹æ–‡ä»¶å¤¹

# æ–‡æœ¬åˆ†å‰²é…ç½®
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50

# ç›‘æ§çš„å­æ–‡ä»¶å¤¹åˆ—è¡¨
MONITORED_SUBFOLDERS = [
    "åŸºç¡€ä¿¡æ¯æ¨¡å—",
    "æ•™å­¦èµ„æºæ¨¡å—", 
    "ç«èµ›ç§‘ç ”æ¨¡å—",
    "ç«èµ›ä¿¡æ¯æ¨¡å—",
    "æ ¡å›­ç”Ÿæ´»æ¨¡å—",
    "è¡Œæ”¿åŠå…¬æ¨¡å—",
    "å­¦ç”ŸæœåŠ¡æ¨¡å—"
]

# æœ¬åœ°æ¨¡å‹è·¯å¾„é…ç½®ï¼ˆç”¨äºç¦»çº¿æ¨¡å¼ï¼‰
LOCAL_MODEL_PATHS = {
    "text2vec": os.path.expanduser("~/.cache/huggingface/hub/models--shibing624--text2vec-base-chinese/snapshots"),
    "text2vec_large": os.path.expanduser("~/.cache/huggingface/hub/models--GanymedeNil--text2vec-large-chinese/snapshots")
}
# ==============================

def initialize_embeddings():
    """åˆå§‹åŒ–Embeddingæ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œæ”¯æŒç¦»çº¿åŠ è½½"""
    # é¦–å…ˆå°è¯•å¯¼å…¥HuggingFaceEmbeddings
    try:
        # ä¼˜å…ˆå°è¯•ä»langchain_huggingfaceå¯¼å…¥ï¼ˆæ–°ç‰ˆï¼‰
        try:
            from langchain_huggingface import HuggingFaceEmbeddings
            logger.info("âœ… ä½¿ç”¨æ–°ç‰ˆlangchain_huggingfaceä¸­çš„HuggingFaceEmbeddings")
        except ImportError:
            # å›é€€åˆ°æ—§ç‰ˆ
            from langchain_community.embeddings import HuggingFaceEmbeddings
            logger.warning("âš ï¸ ä½¿ç”¨æ—§ç‰ˆlangchain_communityä¸­çš„HuggingFaceEmbeddings")
    except ImportError:
        logger.error("âŒ æ— æ³•å¯¼å…¥HuggingFaceEmbeddingsï¼Œè¯·ç¡®ä¿å·²å®‰è£…å¿…è¦çš„åº“")
        logger.info("ğŸ’¡ å°è¯•å®‰è£…: pip install langchain-huggingface sentence-transformers")
        return None
    
    # æ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰ç¼“å­˜çš„æ¨¡å‹
    def find_local_model(snapshot_dir):
        """æŸ¥æ‰¾æœ¬åœ°å¿«ç…§ç›®å½•ä¸­çš„æœ€æ–°æ¨¡å‹"""
        if os.path.exists(snapshot_dir):
            snapshots = [d for d in os.listdir(snapshot_dir) if os.path.isdir(os.path.join(snapshot_dir, d))]
            if snapshots:
                # è¿”å›ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„å¿«ç…§ç›®å½•
                return os.path.join(snapshot_dir, snapshots[0])
        return None
    
    # å°è¯•åŠ è½½æ¨¡å‹çš„é¡ºåº
    model_attempts = [
        # 1. å°è¯•æœ¬åœ°ç¼“å­˜çš„text2vec-base-chinese
        (find_local_model(LOCAL_MODEL_PATHS["text2vec"]), "æœ¬åœ°ç¼“å­˜çš„text2vec-base-chinese"),
        # 2. å°è¯•æœ¬åœ°ç¼“å­˜çš„text2vec-large-chinese
        (find_local_model(LOCAL_MODEL_PATHS["text2vec_large"]), "æœ¬åœ°ç¼“å­˜çš„text2vec-large-chinese"),
        # 3. å°è¯•åœ¨çº¿åŠ è½½text2vec-base-chinese
        ("shibing624/text2vec-base-chinese", "åœ¨çº¿text2vec-base-chinese"),
        # 4. å°è¯•åœ¨çº¿åŠ è½½text2vec-large-chinese
        ("GanymedeNil/text2vec-large-chinese", "åœ¨çº¿text2vec-large-chinese")
    ]
    
    for model_path, model_name in model_attempts:
        if not model_path:
            continue
            
        try:
            logger.info(f"ğŸ” å°è¯•åŠ è½½æ¨¡å‹: {model_name}")
            embeddings = HuggingFaceEmbeddings(
                model_name=model_path,
                model_kwargs={'device': 'cpu', 'local_files_only': model_path.startswith(os.path.expanduser("~"))},
                encode_kwargs={'normalize_embeddings': True},
                # ç¦ç”¨ä»£ç†è®¾ç½®ï¼Œå‡å°‘è¿æ¥é—®é¢˜
                cache_folder=os.path.expanduser("~/.cache/huggingface/hub")
            )
            
            # æµ‹è¯•embeddingæ˜¯å¦æ­£å¸¸å·¥ä½œ
            test_embedding = embeddings.embed_query("æµ‹è¯•")
            if test_embedding and len(test_embedding) > 0:
                logger.info(f"âœ… æˆåŠŸåˆå§‹åŒ–Embeddingæ¨¡å‹: {model_name}")
                return embeddings
            else:
                logger.warning(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼ŒåµŒå…¥å‘é‡ä¸ºç©º: {model_name}")
        except Exception as e:
            logger.warning(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ {model_name}: {str(e)}")
            # å¯¹äºç½‘ç»œè¿æ¥é”™è¯¯ï¼Œæä¾›æ›´æ˜ç¡®çš„æç¤º
            if "timeout" in str(e).lower() or "connection" in str(e).lower():
                logger.info("ğŸ’¡ å»ºè®®ï¼šå…ˆæ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜ï¼Œæˆ–ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")
    
    logger.error("âŒ æ‰€æœ‰æ¨¡å‹åŠ è½½å°è¯•å‡å¤±è´¥")
    logger.info("ğŸ’¡ è¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š")
    logger.info("  1. ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")
    logger.info("  2. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼š")
    logger.info("     - pip install huggingface-hub")
    logger.info("     - huggingface-cli download shibing624/text2vec-base-chinese --local-dir ~/.cache/huggingface/hub/models--shibing624--text2vec-base-chinese")
    
    # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿçš„embeddingå¯¹è±¡ä»¥å…è®¸è„šæœ¬ç»§ç»­è¿è¡Œ
    class MockEmbeddings:
        def embed_documents(self, texts):
            # è¿”å›å›ºå®šç»´åº¦çš„é›¶å‘é‡ä½œä¸ºå ä½ç¬¦
            return [[0.0] * 768 for _ in texts]
        
        def embed_query(self, text):
            # è¿”å›å›ºå®šç»´åº¦çš„é›¶å‘é‡ä½œä¸ºå ä½ç¬¦
            return [0.0] * 768
    
    logger.warning("âš ï¸ ä½¿ç”¨æ¨¡æ‹ŸEmbeddingæ¨¡å‹ï¼ˆä»…ç”¨äºå¼€å‘æµ‹è¯•ï¼‰")
    return MockEmbeddings()

def initialize_vector_db(embeddings):
    """åˆå§‹åŒ–æˆ–è¿æ¥åˆ°Chromaå‘é‡æ•°æ®åº“"""
    try:
        if os.path.exists(VECTOR_DB_PATH):
            logger.info(f"ğŸ”„ è¿æ¥åˆ°ç°æœ‰å‘é‡æ•°æ®åº“: {VECTOR_DB_PATH}")
            vector_db = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
        else:
            logger.info(f"ğŸ“ åˆ›å»ºæ–°å‘é‡æ•°æ®åº“: {VECTOR_DB_PATH}")
            # åˆ›å»ºç©ºæ–‡æ¡£åˆ—è¡¨ä»¥åˆå§‹åŒ–æ•°æ®åº“
            vector_db = Chroma.from_documents([], embedding=embeddings, persist_directory=VECTOR_DB_PATH)
        logger.info("âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        return vector_db
    except Exception as e:
        logger.error(f"âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        raise

class DataUpdateHandler(FileSystemEventHandler):
    """ç›‘æ§æ•°æ®æ–‡ä»¶å¤¹åŠå…¶å­æ–‡ä»¶å¤¹ï¼Œè‡ªåŠ¨æ›´æ–°å‘é‡åº“"""
    
    def __init__(self, vector_db, text_splitter):
        self.vector_db = vector_db
        self.text_splitter = text_splitter
    
    def process_file(self, file_path, file_name):
        """å¤„ç†æ–‡ä»¶ï¼šè¯»å–ã€åˆ†å‰²ã€å‘é‡åŒ–å¹¶æ›´æ–°åˆ°å‘é‡åº“"""
        try:
            logger.info(f"ğŸ“„ æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_name} (è·¯å¾„: {file_path})")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¯è¯»
            if not os.path.exists(file_path):
                logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return None
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            except UnicodeDecodeError:
                # å°è¯•å…¶ä»–ç¼–ç 
                with open(file_path, 'r', encoding='gbk') as f:
                    content = f.read()
            
            if not content.strip():
                logger.warning(f"âš ï¸ æ–‡ä»¶å†…å®¹ä¸ºç©º: {file_name}")
                return None
            
            # åˆ†å‰²æ–‡æœ¬
            chunks = self.text_splitter.split_text(content)
            logger.info(f"âœ‚ï¸  å°†æ–‡ä»¶åˆ†å‰²ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—")
            
            # ç”Ÿæˆå¸¦å…ƒæ•°æ®çš„Documentå¯¹è±¡
            docs = [Document(
                page_content=chunk,
                metadata={
                    "source": file_name,
                    "full_path": file_path,
                    "last_modified": os.path.getmtime(file_path),
                    "folder": os.path.basename(os.path.dirname(file_path))
                }
            ) for chunk in chunks]
            
            return docs
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_name}: {str(e)}")
            return None
    
    def on_created(self, event):
        """æ–°æ–‡ä»¶åˆ›å»ºæ—¶è§¦å‘ï¼ˆåŒ…å«å­æ–‡ä»¶å¤¹ï¼‰"""
        if not event.is_directory and event.src_path.endswith(".txt"):
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨å—ç›‘æ§çš„å­æ–‡ä»¶å¤¹ä¸­
            file_dir = os.path.dirname(event.src_path)
            if any(subfolder in file_dir for subfolder in MONITORED_SUBFOLDERS):
                file_path = event.src_path
                file_name = os.path.basename(file_path)
                logger.info(f"\nğŸ“„ æ£€æµ‹åˆ°æ–°æ–‡ä»¶: {file_name} (è·¯å¾„: {file_path})")
                
                try:
                    time.sleep(1)  # ç­‰å¾…æ–‡ä»¶å®Œå…¨å†™å…¥
                    
                    # å¤„ç†æ–‡ä»¶
                    docs = self.process_file(file_path, file_name)
                    if docs:
                        # æ·»åŠ åˆ°å‘é‡åº“
                        self.vector_db.add_documents(docs)
                        # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„persistæ–¹æ³•
                        if hasattr(self.vector_db, 'persist'):
                            try:
                                self.vector_db.persist()
                            except Exception as e:
                                logger.warning(f"âš ï¸ persistæ–¹æ³•è°ƒç”¨å¤±è´¥ï¼Œå¿½ç•¥: {str(e)}")
                        logger.info(f"âœ… æˆåŠŸæ·»åŠ  {len(docs)} ä¸ªæ–‡æœ¬å—åˆ°å‘é‡åº“ (æ¥æº: {file_name})")
                except Exception as e:
                    logger.error(f"âŒ æ·»åŠ æ–°æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def on_modified(self, event):
        """æ–‡ä»¶ä¿®æ”¹æ—¶è§¦å‘ï¼ˆåŒ…å«å­æ–‡ä»¶å¤¹ï¼‰"""
        if not event.is_directory and event.src_path.endswith(".txt"):
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨å—ç›‘æ§çš„å­æ–‡ä»¶å¤¹ä¸­
            file_dir = os.path.dirname(event.src_path)
            if any(subfolder in file_dir for subfolder in MONITORED_SUBFOLDERS):
                file_path = event.src_path
                file_name = os.path.basename(file_path)
                logger.info(f"\nğŸ”„ æ£€æµ‹åˆ°æ–‡ä»¶ä¿®æ”¹: {file_name} (è·¯å¾„: {file_path})")
                
                try:
                    time.sleep(1)  # ç­‰å¾…æ–‡ä»¶ä¿®æ”¹å®Œæˆ
                    
                    # é€‚é…æ–°ç‰ˆChroma APIï¼Œä½¿ç”¨queryæ¥æ‰¾åˆ°ç›¸å…³æ–‡æ¡£å¹¶åˆ é™¤
                    try:
                        # å°è¯•ä½¿ç”¨collectionå¯¹è±¡çš„deleteæ–¹æ³•ï¼ˆæ–°ç‰ˆAPIï¼‰
                        if hasattr(self.vector_db, 'collection'):
                            logger.info(f"ğŸ—‘ï¸  ä½¿ç”¨æ–°ç‰ˆAPIåˆ é™¤æ—§æ•°æ®: {file_name}")
                            # æŸ¥è¯¢æ‰€æœ‰åŒ¹é…full_pathçš„æ–‡æ¡£
                            results = self.vector_db.similarity_search_by_vector(
                                embedding=[0.0] * 768,  # ä¸´æ—¶å‘é‡ï¼Œåªç”¨äºè¿‡æ»¤
                                k=1000,  # è®¾ç½®ä¸€ä¸ªè¾ƒå¤§çš„å€¼ç¡®ä¿è·å–æ‰€æœ‰åŒ¹é…çš„æ–‡æ¡£
                                filter={"full_path": file_path}
                            )
                            # è·å–æ‰€æœ‰æ–‡æ¡£çš„ids
                            doc_ids = [doc.metadata.get("id") for doc in results if "id" in doc.metadata]
                            # å¦‚æœæœ‰æ–‡æ¡£ï¼Œåˆ é™¤å®ƒä»¬
                            if doc_ids:
                                self.vector_db.collection.delete(ids=doc_ids)
                        # å°è¯•ç›´æ¥ä½¿ç”¨deleteæ–¹æ³•
                        elif hasattr(self.vector_db, 'delete'):
                            logger.info(f"ğŸ—‘ï¸  ä½¿ç”¨deleteæ–¹æ³•åˆ é™¤æ—§æ•°æ®: {file_name}")
                            # å°è¯•ä½¿ç”¨filterå‚æ•°
                            try:
                                self.vector_db.delete(filter={"full_path": file_path})
                            except TypeError:
                                # å¦‚æœä¸æ”¯æŒfilterå‚æ•°ï¼Œå°è¯•è·å–å¹¶åˆ é™¤æ‰€æœ‰æ–‡æ¡£
                                results = self.vector_db.similarity_search("", k=1000)
                                for doc in results:
                                    if doc.metadata.get("full_path") == file_path:
                                        self.vector_db.delete([doc.metadata.get("id")])
                        else:
                            logger.warning(f"âš ï¸  ä¸æ”¯æŒç›´æ¥åˆ é™¤æ“ä½œï¼Œè·³è¿‡åˆ é™¤æ­¥éª¤: {file_name}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ åˆ é™¤æ—§æ•°æ®å¤±è´¥ï¼Œè·³è¿‡åˆ é™¤æ­¥éª¤: {str(e)}")
                    
                    # å¤„ç†å¹¶æ·»åŠ æ–°æ•°æ®
                    docs = self.process_file(file_path, file_name)
                    if docs:
                        self.vector_db.add_documents(docs)
                        # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„persistæ–¹æ³•
                        if hasattr(self.vector_db, 'persist'):
                            try:
                                self.vector_db.persist()
                            except Exception as e:
                                logger.warning(f"âš ï¸ persistæ–¹æ³•è°ƒç”¨å¤±è´¥ï¼Œå¿½ç•¥: {str(e)}")
                        logger.info(f"âœ… æˆåŠŸæ›´æ–°æ–‡ä»¶ {file_name} åˆ°å‘é‡åº“")
                except Exception as e:
                    logger.error(f"âŒ æ›´æ–°æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def on_deleted(self, event):
        """æ–‡ä»¶åˆ é™¤æ—¶è§¦å‘"""
        if not event.is_directory and event.src_path.endswith(".txt"):
            file_path = event.src_path
            file_name = os.path.basename(file_path)
            logger.info(f"\nğŸ—‘ï¸  æ£€æµ‹åˆ°æ–‡ä»¶åˆ é™¤: {file_name}")
            
            try:
                # é€‚é…æ–°ç‰ˆChroma APIï¼Œä½¿ç”¨queryæ¥æ‰¾åˆ°ç›¸å…³æ–‡æ¡£å¹¶åˆ é™¤
                try:
                    # å°è¯•ä½¿ç”¨collectionå¯¹è±¡çš„deleteæ–¹æ³•ï¼ˆæ–°ç‰ˆAPIï¼‰
                    if hasattr(self.vector_db, 'collection'):
                        logger.info(f"ğŸ—‘ï¸  ä½¿ç”¨æ–°ç‰ˆAPIåˆ é™¤æ•°æ®: {file_name}")
                        # æŸ¥è¯¢æ‰€æœ‰åŒ¹é…full_pathçš„æ–‡æ¡£
                        results = self.vector_db.similarity_search_by_vector(
                            embedding=[0.0] * 768,  # ä¸´æ—¶å‘é‡ï¼Œåªç”¨äºè¿‡æ»¤
                            k=1000,  # è®¾ç½®ä¸€ä¸ªè¾ƒå¤§çš„å€¼ç¡®ä¿è·å–æ‰€æœ‰åŒ¹é…çš„æ–‡æ¡£
                            filter={"full_path": file_path}
                        )
                        # è·å–æ‰€æœ‰æ–‡æ¡£çš„ids
                        doc_ids = [doc.metadata.get("id") for doc in results if "id" in doc.metadata]
                        # å¦‚æœæœ‰æ–‡æ¡£ï¼Œåˆ é™¤å®ƒä»¬
                        if doc_ids:
                            self.vector_db.collection.delete(ids=doc_ids)
                    # å°è¯•ç›´æ¥ä½¿ç”¨deleteæ–¹æ³•
                    elif hasattr(self.vector_db, 'delete'):
                        logger.info(f"ğŸ—‘ï¸  ä½¿ç”¨deleteæ–¹æ³•åˆ é™¤æ•°æ®: {file_name}")
                        # å°è¯•ä½¿ç”¨filterå‚æ•°
                        try:
                            self.vector_db.delete(filter={"full_path": file_path})
                        except TypeError:
                            # å¦‚æœä¸æ”¯æŒfilterå‚æ•°ï¼Œå°è¯•è·å–å¹¶åˆ é™¤æ‰€æœ‰æ–‡æ¡£
                            results = self.vector_db.similarity_search("", k=1000)
                            for doc in results:
                                if doc.metadata.get("full_path") == file_path:
                                    self.vector_db.delete([doc.metadata.get("id")])
                    else:
                        logger.warning(f"âš ï¸  ä¸æ”¯æŒç›´æ¥åˆ é™¤æ“ä½œï¼Œè·³è¿‡åˆ é™¤æ­¥éª¤: {file_name}")
                except Exception as e:
                    logger.warning(f"âš ï¸ åˆ é™¤æ•°æ®å¤±è´¥: {str(e)}")
                    
                # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„persistæ–¹æ³•
                if hasattr(self.vector_db, 'persist'):
                    try:
                        self.vector_db.persist()
                    except Exception as e:
                        logger.warning(f"âš ï¸ persistæ–¹æ³•è°ƒç”¨å¤±è´¥ï¼Œå¿½ç•¥: {str(e)}")
                logger.info(f"âœ… æˆåŠŸå¤„ç†æ–‡ä»¶ {file_name} çš„åˆ é™¤äº‹ä»¶")
            except Exception as e:
                logger.error(f"âŒ å¤„ç†åˆ é™¤äº‹ä»¶å¤±è´¥: {str(e)}")

def validate_monitored_folders():
    """éªŒè¯å—ç›‘æ§çš„å­æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨"""
    valid_folders = []
    missing_folders = []
    
    for subfolder in MONITORED_SUBFOLDERS:
        folder_path = os.path.join(DATA_FOLDER, subfolder)
        if os.path.exists(folder_path) and os.path.isdir(folder_path):
            valid_folders.append(subfolder)
        else:
            missing_folders.append(subfolder)
    
    if valid_folders:
        logger.info(f"âœ… æ‰¾åˆ° {len(valid_folders)} ä¸ªæœ‰æ•ˆå­æ–‡ä»¶å¤¹")
        for folder in valid_folders:
            logger.info(f"   - {folder}")
    
    if missing_folders:
        logger.warning(f"âš ï¸  æœªæ‰¾åˆ° {len(missing_folders)} ä¸ªå­æ–‡ä»¶å¤¹:")
        for folder in missing_folders:
            logger.warning(f"   - {folder}")
    
    return valid_folders

def main():
    """ä¸»å‡½æ•°ï¼šåˆå§‹åŒ–å·¥å…·å¹¶å¯åŠ¨ç›‘æ§"""
    logger.info(f"ğŸ¯ å¯åŠ¨æ•°æ®è‡ªåŠ¨æ›´æ–°ç›‘æ§ç³»ç»Ÿ")
    logger.info(f"ğŸ“‚ æ•°æ®ä»“åº“è·¯å¾„: {DATA_FOLDER}")
    logger.info(f"ğŸ’¾ å‘é‡æ•°æ®åº“è·¯å¾„: {VECTOR_DB_PATH}")
    
    # éªŒè¯æ•°æ®ä»“åº“ä¸»æ–‡ä»¶å¤¹
    if not os.path.exists(DATA_FOLDER) or not os.path.isdir(DATA_FOLDER):
        logger.error(f"âŒ æ•°æ®ä»“åº“æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {DATA_FOLDER}")
        return
    
    # éªŒè¯å—ç›‘æ§çš„å­æ–‡ä»¶å¤¹
    valid_folders = validate_monitored_folders()
    if not valid_folders:
        logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å­æ–‡ä»¶å¤¹ï¼Œç¨‹åºé€€å‡º")
        return
    
    # åˆå§‹åŒ–Embeddingæ¨¡å‹
    embeddings = initialize_embeddings()
    if embeddings is None:
        logger.error("âŒ æ— æ³•åˆå§‹åŒ–Embeddingæ¨¡å‹ï¼Œç¨‹åºé€€å‡º")
        return
    
    # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = CharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separator="\n",
        add_start_index=True
    )
    logger.info(f"âœ… æ–‡æœ¬åˆ†å‰²å™¨åˆå§‹åŒ–å®Œæˆ (chunk_size={CHUNK_SIZE}, chunk_overlap={CHUNK_OVERLAP})")
    
    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    vector_db = initialize_vector_db(embeddings)
    
    # åˆå§‹åŒ–äº‹ä»¶å¤„ç†å™¨
    event_handler = DataUpdateHandler(vector_db, text_splitter)
    
    # åˆå§‹åŒ–ç›‘æ§å™¨
    observer = Observer()
    
    # ç›‘æ§æ ¹æ–‡ä»¶å¤¹åŠå…¶æ‰€æœ‰å­æ–‡ä»¶å¤¹
    observer.schedule(event_handler, path=DATA_FOLDER, recursive=True)
    
    # å¯åŠ¨ç›‘æ§
    observer.start()
    logger.info("ğŸš€ å¼€å§‹ç›‘æ§æ–‡ä»¶å˜åŒ–...")
    logger.info("ğŸ’¡ æ“ä½œè¯´æ˜:")
    logger.info("   1. æ–°å¢/ä¿®æ”¹/åˆ é™¤ .txt æ–‡ä»¶å°†è‡ªåŠ¨åŒæ­¥åˆ°å‘é‡åº“")
    logger.info("   2. æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
    
    try:
        # æŒç»­è¿è¡Œç›‘æ§
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ æ¥æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢ç›‘æ§...")
    except Exception as e:
        logger.error(f"âŒ ç›‘æ§è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
    finally:
        # åœæ­¢ç›‘æ§
        try:
            observer.stop()
            observer.join()
            logger.info("âœ… ç›‘æ§å·²åœæ­¢")
        except Exception as e:
            logger.error(f"âŒ åœæ­¢ç›‘æ§æ—¶å‡ºé”™: {str(e)}")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)