import os
import shutil
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import CharacterTextSplitter
# å…³é”®ä¿®æ­£ï¼š1.0.0ç‰ˆæœ¬Documentåœ¨langchain_core.documentsä¸‹
from langchain_core.documents import Document

def load_documents_with_source(data_path):
    """åŠ è½½æ–‡æ¡£å¹¶ä¿å­˜æ¥æºï¼ˆæ–‡ä»¶åï¼‰"""
    documents = []
    txt_files = []
    
    # éå†æ‰€æœ‰txtæ–‡ä»¶
    for root, dirs, files in os.walk(data_path):
        for file in files:
            if file.endswith(".txt"):
                file_path = os.path.join(root, file)
                txt_files.append(file_path)
    
    print(f"æ‰¾åˆ° {len(txt_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶")
    
    for file_path in txt_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–æ–‡ä»¶åä½œä¸ºæ¥æºï¼ˆå¦‚â€œå¥–å­¦é‡‘.txtâ€ï¼‰
            source = os.path.basename(file_path)
            
            # åˆ›å»ºDocumentå¯¹è±¡ï¼Œå¼ºåˆ¶ä¿å­˜æ¥æºä¿¡æ¯
            doc = Document(
                page_content=content,
                metadata={"source": source}
            )
            documents.append(doc)
            print(f"   âœ… åŠ è½½æˆåŠŸï¼š{source}")
        
        except Exception as e:
            print(f"   âŒ åŠ è½½å¤±è´¥ {os.path.basename(file_path)}: {str(e)}")
    
    return documents

def main():
    print("ğŸ¯ é‡æ–°æ„å»ºå¸¦æ¥æºä¿¡æ¯çš„å‘é‡æ•°æ®åº“ï¼ˆé€‚é…langchain_core==1.0.0ï¼‰")
    
    # 1. é…ç½®è·¯å¾„
    data_path = "æ±Ÿè¥¿å·¥ä¸šå·¥ç¨‹èŒä¸šæŠ€æœ¯å­¦é™¢_æ•°æ®ä»“åº“"
    db_path = "./vector_db"  # è¦†ç›–æ—§å‘é‡åº“
    
    # 2. å¼ºåˆ¶åˆ é™¤æ—§å‘é‡åº“ï¼ˆå½»åº•æ¸…ç†ï¼‰
    if os.path.exists(db_path):
        shutil.rmtree(db_path)
        print("âœ… å·²åˆ é™¤æ—§å‘é‡æ•°æ®åº“")
    
    # 3. åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨å’ŒåµŒå…¥æ¨¡å‹
    text_splitter = CharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separator="\n"
    )
    embeddings = HuggingFaceEmbeddings(
        model_name="GanymedeNil/text2vec-large-chinese"
    )
    print("âœ… æ–‡æœ¬å·¥å…·å’ŒåµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    # 4. åŠ è½½æ–‡æ¡£ï¼ˆå¸¦æ–‡ä»¶åæ¥æºï¼‰
    print("\nğŸ“„ åŠ è½½æ–‡æ¡£...")
    documents = load_documents_with_source(data_path)
    if len(documents) == 0:
        print("âŒ æœªåŠ è½½åˆ°ä»»ä½•æ–‡æ¡£")
        return
    
    # 5. åˆ†å‰²æ–‡æœ¬ï¼ˆæ¯ä¸ªå—éƒ½ä¿ç•™æ¥æºï¼‰
    print("\nâœ‚ï¸  åˆ†å‰²æ–‡æœ¬...")
    all_chunks = []
    for doc in documents:
        chunks = text_splitter.split_text(doc.page_content)
        for chunk in chunks:
            chunk_doc = Document(
                page_content=chunk,
                metadata={"source": doc.metadata["source"]}  # åˆ†å‰²åä¸ä¸¢å¤±æ¥æº
            )
            all_chunks.append(chunk_doc)
    print(f"âœ… æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…± {len(all_chunks)} ä¸ªå—")
    
    # 6. æ„å»ºæ–°å‘é‡åº“
    print("\nğŸ› ï¸  æ„å»ºå‘é‡æ•°æ®åº“...")
    vector_db = Chroma.from_documents(
        documents=all_chunks,
        embedding=embeddings,
        persist_directory=db_path
    )
    print("âœ… æ–°å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
    print(f"ğŸ“ æ•°æ®åº“ä½ç½®ï¼š{db_path}")

if __name__ == "__main__":
    main()