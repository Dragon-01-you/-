import requests
import json
import time

# ç›´æ¥å¤åˆ¶APIé…ç½®ä¿¡æ¯ï¼Œç»•è¿‡HTTPå±‚ç›´æ¥æµ‹è¯•
MODEL_CONFIG = {
    "api_base": "https://api-inference.huggingface.co/models/THUDM/chatglm-6b",
    "api_key": "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",
    "model_name": "chatglm-6b",
    "timeout": 100
}

# æµ‹è¯•é—®é¢˜åˆ—è¡¨
TEST_QUESTIONS = [
    "æ±Ÿè¥¿å·¥ä¸šå·¥ç¨‹èŒä¸šæŠ€æœ¯å­¦é™¢çš„å†å²æœ‰å¤šä¹…äº†ï¼Ÿ",
    "å›¾ä¹¦é¦†çš„å¼€æ”¾æ—¶é—´æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ",
    "å¥–å­¦é‡‘ç”³è¯·éœ€è¦ä»€ä¹ˆæ¡ä»¶ï¼Ÿ",
    "å­¦æ ¡æœ‰å“ªäº›è‘—åçš„æ•™å¸ˆï¼Ÿ"
]

def call_llm_api_direct(prompt):
    """ç›´æ¥è°ƒç”¨å¤§æ¨¡å‹APIçš„å‡½æ•°ï¼Œç»•è¿‡HTTPæœåŠ¡å™¨"""
    try:
        # æ„å»ºæ¶ˆæ¯
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤º
        system_prompt = """
        ä½ æ˜¯æ±Ÿè¥¿å·¥ä¸šå·¥ç¨‹èŒä¸šæŠ€æœ¯å­¦é™¢çš„æ™ºèƒ½é—®ç­”åŠ©æ‰‹ã€‚
        è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œå¯¹è¯å†å²ï¼Œç”¨è‡ªç„¶ã€å‹å¥½çš„è¯­è¨€å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
        å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·å¦ç‡è¡¨ç¤ºï¼Œå¹¶å»ºè®®ç”¨æˆ·è”ç³»å­¦æ ¡ç›¸å…³éƒ¨é—¨ã€‚
        å›ç­”è¦ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡ºã€‚
        """
        messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ å½“å‰é—®é¢˜
        messages.append({"role": "user", "content": prompt})
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        payload = {
            "model": MODEL_CONFIG["model_name"],
            "messages": messages,
            "temperature": 0.7,
            "max_tokens": 1000
        }
        
        # å‘é€è¯·æ±‚
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {MODEL_CONFIG['api_key']}"
        }
        
        print(f"æ­£åœ¨è°ƒç”¨ChatGLM-6B API...")
        start_time = time.time()
        response = requests.post(
            MODEL_CONFIG["api_base"],
            headers=headers,
            json=payload,
            timeout=MODEL_CONFIG["timeout"]
        )
        
        end_time = time.time()
        print(f"APIè°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {(end_time - start_time):.2f}ç§’")
        
        if response.status_code != 200:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            print(f"å“åº”å†…å®¹: {response.text}")
            return False, f"APIè°ƒç”¨å¤±è´¥: {response.status_code}"
        
        # è§£æå“åº”
        result = response.json()
        if "choices" in result and result["choices"]:
            answer = result["choices"][0]["message"]["content"]
            print(f"âœ… æˆåŠŸè·å–å›ç­”")
            print(f"å›ç­”å†…å®¹: {answer}")
            return True, answer
        else:
            print(f"âŒ APIè¿”å›æ ¼å¼å¼‚å¸¸")
            print(f"å“åº”å†…å®¹: {result}")
            return False, "APIè¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸"
            
    except Exception as e:
        print(f"âŒ APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
        return False, f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}"

def main():
    print("=== ChatGLM-6B APIç›´æ¥æµ‹è¯•å·¥å…· ===")
    print(f"APIç«¯ç‚¹: {MODEL_CONFIG['api_base']}")
    print(f"æ¨¡å‹åç§°: {MODEL_CONFIG['model_name']}")
    print()
    
    success_count = 0
    
    for i, question in enumerate(TEST_QUESTIONS):
        print(f"\né—®é¢˜ {i+1}/{len(TEST_QUESTIONS)}: {question}")
        print("-" * 60)
        
        success, answer = call_llm_api_direct(question)
        if success:
            success_count += 1
        
        print("=" * 60)
    
    # æ€»ç»“
    print(f"\næµ‹è¯•æ€»ç»“:")
    print(f"æ€»é—®é¢˜æ•°: {len(TEST_QUESTIONS)}")
    print(f"æˆåŠŸæ•°: {success_count}")
    print(f"æˆåŠŸç‡: {(success_count / len(TEST_QUESTIONS) * 100):.1f}%")
    
    if success_count == len(TEST_QUESTIONS):
        print("\nğŸ‰ ChatGLM-6B APIè°ƒç”¨æµ‹è¯•æˆåŠŸï¼")
    else:
        print("\nâš ï¸ éƒ¨åˆ†APIè°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥ã€‚")

if __name__ == "__main__":
    main()